import os
import io
import re
import json
import numpy as np
import cv2
import streamlit as st
import torch
import folium
from typing import Optional
from datetime import datetime
from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
from geopy.geocoders import Nominatim
from streamlit_folium import st_folium

# AI ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from paddleocr import PaddleOCR
from sqlmodel import Field, Session, SQLModel, create_engine, select
from transformers import (
    AutoProcessor, AutoModelForImageClassification, 
    AutoTokenizer, AutoModelForSeq2SeqLM,
    DetrImageProcessor, DetrForObjectDetection
)
from sentence_transformers import SentenceTransformer
from kiwipiepy import Kiwi

# í™˜ê²½ ì„¤ì •
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
# [DEBUG] í•˜ë“œì›¨ì–´ ê°€ì† ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¶©ëŒì„ ë°©ì§€í•˜ëŠ” ì„¤ì •
os.environ['DNNL_MAX_CPU_ISA'] = 'AVX2'

# ---------------------------------------------------------
# 1. DB ëª¨ë¸ ë° ì´ˆê¸°í™”
# ---------------------------------------------------------
class Document(SQLModel, table=True):
    __table_args__ = {"extend_existing": True} 
    id: Optional[int] = Field(default=None, primary_key=True)
    filename: str
    doc_type: str 
    content: str 
    summary: str
    keywords: str
    structured_data: str 
    upload_date: datetime = Field(default_factory=datetime.now)
    image_data: bytes
    embedding: Optional[str] = None

engine = create_engine("sqlite:///archive.db")
SQLModel.metadata.create_all(engine)
kiwi = Kiwi()

# ---------------------------------------------------------
# 2. AI ëª¨ë¸ ë¡œë”© (ìºì‹±)
# ---------------------------------------------------------
@st.cache_resource
def load_all_models():
    ocr = PaddleOCR(lang='korean', show_log=False)
    dit_p = AutoProcessor.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    dit_m = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    obj_p = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
    obj_m = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
    sum_t = AutoTokenizer.from_pretrained("gogamza/kobart-summarization")
    sum_m = AutoModelForSeq2SeqLM.from_pretrained("gogamza/kobart-summarization")
    emb_m = SentenceTransformer("jhgan/ko-sroberta-multitask")
    return (dit_p, dit_m, ocr, obj_p, obj_m, sum_t, sum_m, emb_m)

# ---------------------------------------------------------
# 3. ë³´ì¡° ë¶„ì„ í•¨ìˆ˜ (ì •ê·œí‘œí˜„ì‹ ì˜ìˆ˜ì¦ ì¶”ì¶œ ì¶”ê°€)
# ---------------------------------------------------------

def extract_receipt_info(text):
    """ì˜ìˆ˜ì¦ í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ ì •í˜•í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ì‚¬ì—…ì ë²ˆí˜¸ ì¶”ì¶œ
    biz_num_match = re.search(r'\d{3}[ -]?\d{2}[ -]?\d{5}', text)
    # ë‚ ì§œ 
    date_match = re.search(r'\d{4}-\d{2}-\d{2}', text)
    # ê¸ˆì•¡
    total_price_match = re.search(r'(?:í•©\s*ê³„|ê²°ì œê¸ˆì•¡|ì´ì•¡)\s*[:\s]*([\d\s,]+)', text)
    # í’ˆëª© 
    item_pattern = r'(\d{2,})?\s*([ê°€-í£A-Z\(\)\[\]][ê°€-í£A-Z0-9\(\)\[\]\-~ ]+?)(?=\s+\d+)'
    items = re.findall(item_pattern, text)
    
    # --- ì¶œë ¥ í™•ì¸ êµ¬ê°„ (ì¶”ê°€ëœ ë¶€ë¶„) ---
    # print(f"\n[DEBUG] 1. ì •ê·œì‹ ì¶”ì¶œ ê²°ê³¼ (items): {items}") 
    # -------------------------------

    res = []
    if biz_num_match: res.append(f"ğŸ¢ ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸: {biz_num_match.group()}")
    print(f"\n[DEBUG] ì‚¬ì—…ì: {biz_num_match.group()}") 
    if date_match: res.append(f"ğŸ“… ë‚ ì§œ: {date_match.group()}")
    if total_price_match:
        price = total_price_match.group(1).replace(" ", "").replace(",", "").strip()
        res.append(f"ğŸ’° ì´í•©ê³„: {int(price):,}ì›")
    
    if items:
        valid_items = []
        # 1. ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ëŒ€í­ ê°•í™” (OCR ì˜¤íƒ€ ëŒ€ì‘)
        stopwords = [
        # ê²°ì œ ê´€ë ¨
        'ë¬¼í’ˆê°€ì•¡', 'ê³¼ì„¸', 'ë¶€ê°€ì„¸', 'ë¶€ê°€ì„œ', 'ìƒí’ˆê°€ê²©', 'í•©ê³„', 'ê¸ˆì•¡', 'ìˆ˜ëŸ‰', 'ë‹¨ê°€',
        # ì í¬/ì£¼ì†Œ ê´€ë ¨ (ì´ë²ˆì— ì¶”ê°€!)
        'ì´ë§ˆíŠ¸', 'KMART', 'ëŒ€í•œë¯¼êµ­', 'ê³ ì–‘ì‹œ', 'ë•ì´ë™', 'ì£¼ì†Œ', 'ëŒ€í‘œì', 'ì „í™”',
        # ì•ˆë‚´ ë¬¸êµ¬ ê´€ë ¨ (ì´ë²ˆì— ì¶”ê°€!)
        'í™˜ë¶ˆ', 'í™˜ë¬¼', 'êµí™˜', 'í¸ë¦¬', 'ë“±ë¡', 'ì˜ìˆ˜ì¦', 'ë¬¸ì˜', 'ê°ì‚¬'
    ]
        
        for it in items:
            raw_name = it[1].strip()
            
            # [í•µì‹¬ ë¡œì§] ê³µë°±ì„ ì œê±°í•œ ìƒíƒœì—ì„œ ë¹„êµí•©ë‹ˆë‹¤.
            # 'í•© ê³„' -> 'í•©ê³„'ë¡œ ë³€í™˜í•´ì„œ ì²´í¬í•˜ë‹ˆê¹Œ í›¨ì”¬ ì˜ ê±¸ë ¤ìš”!
            clean_check_name = raw_name.replace(" ", "")
            
            # ë¶ˆìš©ì–´ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ íŒ¨ìŠ¤!
            if any(stop.replace(" ", "") in clean_check_name for stop in stopwords):
                continue
            
            valid_items.append(raw_name)
        
        # ì¤‘ë³µ ì œê±° (set í™œìš©)
        valid_items = list(dict.fromkeys(valid_items))

        if valid_items:
            item_str = f"ğŸ›’ í’ˆëª©: {valid_items[0]} ë“± {len(valid_items)}ê±´"
            res.append(item_str)
            print(f"[DEBUG] ìµœì¢… ì •ì œëœ í’ˆëª©ë“¤: {valid_items}")
            
    return " | ".join(res) if res else "ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨"


def extract_photo_metadata(image):
    metadata = {'width': image.width, 'height': image.height, 'camera_model': 'ì •ë³´ ì—†ìŒ', 'taken_date': 'ì •ë³´ ì—†ìŒ', 'location_address': 'ì •ë³´ ì—†ìŒ', 'lat': None, 'lng': None}
    try:
        exif_data = image._getexif()
        if exif_data:
            for tag_id, value in exif_data.items():
                tag = TAGS.get(tag_id, tag_id)
                if tag == "Model": metadata['camera_model'] = str(value).strip()
                elif tag in ["DateTime", "DateTimeOriginal"]: metadata['taken_date'] = str(value).replace(':', '-', 2)
                elif tag == "GPSInfo" and isinstance(value, dict):
                    gps_data = {GPSTAGS.get(t, t): value[t] for t in value}
                    if 'GPSLatitude' in gps_data and 'GPSLongitude' in gps_data:
                        def to_decimal(dms, ref):
                            d, m, s = [float(x) for x in dms]
                            res = d + m/60.0 + s/3600.0
                            return -res if ref in ['S', 'W'] else res
                        metadata['lat'] = to_decimal(gps_data['GPSLatitude'], gps_data['GPSLatitudeRef'])
                        metadata['lng'] = to_decimal(gps_data['GPSLongitude'], gps_data['GPSLongitudeRef'])
                        try:
                            geolocator = Nominatim(user_agent="geo_archive_v4")
                            loc = geolocator.reverse(f"{metadata['lat']}, {metadata['lng']}", language='ko')
                            if loc: metadata['location_address'] = loc.address
                        except: pass
    except: pass
    return metadata

# ---------------------------------------------------------
# 4. ë©”ì¸ í”„ë¡œì„¸ì‹± í•¨ìˆ˜ (ìˆ˜ì •ë¨)
# ---------------------------------------------------------
def process_document(uploaded_file, models):
    (dit_p, dit_m, ocr, obj_p, obj_m, sum_t, sum_m, emb_m) = models
    file_bytes = uploaded_file.read()
    raw_img = Image.open(io.BytesIO(file_bytes))
    orig_img = raw_img.convert("RGB")
    
    # 1. ë¬¸ì„œ ë¶„ë¥˜
    inputs = dit_p(images=orig_img, return_tensors="pt")
    label = dit_m.config.id2label[dit_m(**inputs).logits.argmax(-1).item()].lower()
    
    # 2. OCR ìˆ˜í–‰ (ë¶„ë¥˜ ë° í…ìŠ¤íŠ¸ í™•ë³´)
    img_cv = cv2.cvtColor(np.array(orig_img), cv2.COLOR_RGB2BGR)
    ocr_res = ocr.ocr(img_cv, cls=False)
    full_text = " ".join([line[1][0] for line in ocr_res[0]]) if ocr_res and ocr_res[0] else ""

    # ë¬¸ì„œ/ì‚¬ì§„ íŒë³„
    is_doc = any(x in label for x in ['receipt', 'invoice', 'form', 'letter']) or len(full_text) > 40
    
    if is_doc:
        doc_type = "Document"
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì´ì§„í™” ë“±)
        # ì•ˆë…•í•˜ì„¸ìš”? ì§„í™”ì”¨? 
        height, width = img_cv.shape[:2]
        img_cv_up = cv2.resize(img_cv, (width * 2, height * 2), interpolation=cv2.INTER_LANCZOS4)
        gray = cv2.cvtColor(img_cv_up, cv2.COLOR_BGR2GRAY)
        _, processed_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # 3. ìš”ì•½ ë¡œì§ ë¶„ê¸° (ì˜ìˆ˜ì¦ vs ì¼ë°˜ ë¬¸ì„œ)
        receipt_summary = extract_receipt_info(full_text)
        
        if ('receipt' in label or 'invoice' in label) and receipt_summary:
            # ì˜ìˆ˜ì¦ì´ë©´ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •í˜•í™”ëœ ìš”ì•½ ì‚¬ìš©
            final_summary = f"ğŸ§¾ [ì˜ìˆ˜ì¦] {receipt_summary}"
        else:
            # ì¼ë°˜ ë¬¸ì„œë©´ KoBART AI ìš”ì•½ ì‚¬ìš©
            try:
                s_inputs = sum_t([full_text], max_length=128, return_tensors="pt", truncation=True)
                s_ids = sum_m.generate(s_inputs["input_ids"], num_beams=4, max_length=128)
                final_summary = sum_t.decode(s_ids[0], skip_special_tokens=True).strip()
            except:
                final_summary = f"{full_text[:30]}..."

        kw_list = [t.form for t in kiwi.tokenize(full_text) if t.tag in ['NNG', 'NNP']]
        final_keywords = ", ".join(list(dict.fromkeys(kw_list))[:10])
        structured_data = {}
    else:
        doc_type = "Photo"
        processed_img = np.array(orig_img)
        meta = extract_photo_metadata(raw_img)
        # ê°ì²´ íƒì§€
        obj_inputs = obj_p(images=orig_img, return_tensors="pt")
        obj_outputs = obj_m(**obj_inputs)
        target_sizes = torch.tensor([orig_img.size[::-1]])
        results = obj_p.post_process_object_detection(obj_outputs, target_sizes=target_sizes, threshold=0.7)[0]
        objs = list(set([obj_m.config.id2label[l.item()] for l in results["labels"]]))
        
        final_keywords = generate_photo_keywords(meta, objs)
        final_summary = f"ğŸ“¸ [{meta['taken_date']}] ì´¬ì˜ ì‚¬ì§„. íƒì§€: {', '.join(objs)}"
        structured_data = {'exif': meta, 'objects': objs}

    embedding = emb_m.encode(full_text + " " + final_keywords).tolist()
    return (doc_type, full_text, final_summary, final_keywords, structured_data, file_bytes, embedding, processed_img)

# ---------------------------------------------------------
# 5. UI (ê¸°ì¡´ê³¼ ë™ì¼)
# ---------------------------------------------------------
st.set_page_config(layout="wide", page_title="AI Multi-Archive")
st.title("ğŸŒŸ ë©€í‹°ëª¨ë‹¬ AI í†µí•© ì•„ì¹´ì´ë¸Œ")

models = load_all_models()
t1, t2, t3, t4 = st.tabs(["ğŸ“¤ ì—…ë¡œë“œ", "ğŸ” ê²€ìƒ‰", "ğŸ“ ì•„ì¹´ì´ë¸Œ", "ğŸ“ ì§€ë„"])

with t1:
    file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=['jpg', 'png', 'jpeg'])
    if file:
        if "res" not in st.session_state or st.session_state.get("fname") != file.name:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                st.session_state.res = process_document(file, models)
                st.session_state.fname = file.name
        
        r = st.session_state.res
        col1, col2 = st.columns(2)
        col1.image(r[5], caption="ì›ë³¸")
        col2.image(r[7], caption="OCR ì „ì²˜ë¦¬ ê²°ê³¼")
        
        st.write(f"**ë¶„ë¥˜:** {r[0]} | **í‚¤ì›Œë“œ:** `{r[3]}`")
        st.info(f"**ìš”ì•½:** {r[2]}")
        
        if st.button("ğŸš€ ìµœì¢… ì €ì¥", type="primary"):
            with Session(engine) as session:
                new_doc = Document(filename=file.name, doc_type=r[0], content=r[1], 
                                   summary=r[2], keywords=r[3], 
                                   structured_data=json.dumps(r[4], ensure_ascii=False),
                                   image_data=r[5], embedding=json.dumps(r[6]))
                session.add(new_doc); session.commit()
            st.success("ì €ì¥ ì™„ë£Œ!")

with t2:
    q = st.text_input("ê²€ìƒ‰ì–´ (ê°ì²´, ì¥ì†Œ, ë‚´ìš© ë“±)")
    if q:
        with Session(engine) as session:
            results = session.exec(select(Document).where((Document.content.contains(q)) | (Document.keywords.contains(q)))).all()
            for d in results:
                with st.expander(f"ğŸ“„ {d.filename} ({d.doc_type})"):
                    sc1, sc2 = st.columns([1, 3])
                    sc1.image(d.image_data)
                    sc2.write(f"**ìš”ì•½:** {d.summary}")
                    sc2.write(f"**í‚¤ì›Œë“œ:** `{d.keywords}`")

with t3:
    with Session(engine) as session:
        items = session.exec(select(Document).order_by(Document.upload_date.desc())).all()
        for item in items:
            with st.container(border=True):
                c1, c2 = st.columns([1, 4])
                c1.image(item.image_data)
                c2.write(f"**{item.filename}** ({item.doc_type})")
                c2.caption(f"ìš”ì•½: {item.summary} | í‚¤ì›Œë“œ: {item.keywords}")
                if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{item.id}"):
                    session.delete(item); session.commit(); st.rerun()

with t4:
    st.header("ğŸ“ ì‚¬ì§„ ì´¬ì˜ ìœ„ì¹˜")
    with Session(engine) as session:
        # ì˜¤ë¥˜ í•´ê²°: st.all_docsê°€ ì•„ë‹ˆë¼ ë³€ìˆ˜ì— ë°ì´í„°ë¥¼ ë‹´ì•„ í•¨ìˆ˜ì— ì „ë‹¬í•´ì•¼ í•¨
        all_docs = session.exec(select(Document)).all()
        if all_docs:
            # display_photo_locations í•¨ìˆ˜ë¥¼ í˜¸ì¶œ (all_docs ì¸ì ì „ë‹¬)
            # (í•´ë‹¹ í•¨ìˆ˜ ë‚´ì—ì„œ lat/lng ì¶”ì¶œ ë¡œì§ì´ d.structured_dataë¥¼ íŒŒì‹±í•˜ë„ë¡ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ í•„ìš”)
            st.info(f"í˜„ì¬ {len(all_docs)}ê°œì˜ ë°ì´í„°ê°€ ì•„ì¹´ì´ë¸Œì— ìˆìŠµë‹ˆë‹¤.")
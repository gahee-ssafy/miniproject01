import os
import io
import re
import json
import numpy as np
import cv2
import streamlit as st
import torch
import folium
from typing import Optional
from datetime import datetime
from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
from geopy.geocoders import Nominatim
from streamlit_folium import st_folium

# AI ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from paddleocr import PaddleOCR
from sqlmodel import Field, Session, SQLModel, create_engine, select
from transformers import (
    AutoProcessor, AutoModelForImageClassification, 
    AutoTokenizer, AutoModelForSeq2SeqLM,
    DetrImageProcessor, DetrForObjectDetection
)
from sentence_transformers import SentenceTransformer
from kiwipiepy import Kiwi

# í™˜ê²½ ì„¤ì •
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
os.environ['DNNL_MAX_CPU_ISA'] = 'AVX2'

# ---------------------------------------------------------
# 1. DB: ì—†ìœ¼ë©´ ë§Œë“¤ê³ , ìˆìœ¼ë©´ ë†”ë‘¬ë¼
# ---------------------------------------------------------
class Document(SQLModel, table=True):
    __table_args__ = {"extend_existing": True} 
    id: Optional[int] = Field(default=None, primary_key=True)
    filename: str
    doc_type: str 
    content: str 
    summary: str
    keywords: str
    structured_data: str 
    upload_date: datetime = Field(default_factory=datetime.now)
    image_data: bytes
    embedding: Optional[str] = None

engine = create_engine("sqlite:///archive.db")
SQLModel.metadata.create_all(engine)
kiwi = Kiwi() # Q1 ì´ê±° ì™œ í•˜ì§€? 
# í‚¤ìœ„ëŠ” "í•œêµ­ì–´" í˜•íƒœì†Œ ë¶„ì„ê¸°ì…ë‹ˆë‹¤. 
# ì˜ìˆ˜ì¦ì´ë‚˜ ë¬¸ì„œì—ì„œ ëª…ì‚¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. 
# ì˜ˆë¥¼ ë“¤ì–´, "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œ S21 128GB"ë¼ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´, í‚¤ìœ„ëŠ” "ì‚¼ì„±ì „ì", "ê°¤ëŸ­ì‹œ", "S21", "128GB" ê°™ì€ ëª…ì‚¬ë“¤ì„ ì¶”ì¶œí•´ì¤ë‹ˆë‹¤. 
# ì´ë ‡ê²Œ ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì€ ê²€ìƒ‰ì´ë‚˜ ë¶„ë¥˜ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# Q2 ì™œ ì²˜ìŒì— í•´ì•¼í•˜ëŠ”ë°? 
# í‚¤ìœ„ ê°ì²´ë¥¼ ë¯¸ë¦¬ ìƒì„±í•´ë‘ë©´, ì´í›„ì— í˜•íƒœì†Œ ë¶„ì„ì´ í•„ìš”í•  ë•Œë§ˆë‹¤ ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

# ---------------------------------------------------------
# 2. AI ëª¨ë¸ ë¡œë”© (ìºì‹±)
# ---------------------------------------------------------
@st.cache_resource
def load_all_models():
    ocr = PaddleOCR(lang='korean', show_log=False)
    dit_p = AutoProcessor.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    dit_m = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    obj_p = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
    obj_m = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
    sum_t = AutoTokenizer.from_pretrained("gogamza/kobart-summarization")
    sum_m = AutoModelForSeq2SeqLM.from_pretrained("gogamza/kobart-summarization")
    emb_m = SentenceTransformer("jhgan/ko-sroberta-multitask")
    return (dit_p, dit_m, ocr, obj_p, obj_m, sum_t, sum_m, emb_m)

# ---------------------------------------------------------
# 3. ë³´ì¡° ë¶„ì„ í•¨ìˆ˜ (ì •ê·œí‘œí˜„ì‹ ì˜ìˆ˜ì¦ ì¶”ì¶œ ì¶”ê°€)
# ---------------------------------------------------------
# ì˜ìˆ˜ì¦ ì¶”ì¶œ
def extract_receipt_info(text):    
    # ì‚¬ì—…ì ë²ˆí˜¸ ì¶”ì¶œ
    biz_num_match = re.search(r'\d{3}[-\s]?\d{2}[-\s]?\d{5}', text)
    # ë‚ ì§œ 
    date_match = re.search(r'\d{4}-\d{2}-\d{2}', text)
    # ê¸ˆì•¡
    total_price_match = re.search(r'(?:í•©\s*ê³„|ê²°ì œê¸ˆì•¡|ì´ì•¡)\s*[:\s]*([\d\s,]+)', text)
    # í’ˆëª© 
    item_pattern = r'(\d{2,})?\s*([ê°€-í£A-Z\(\)\[\]][ê°€-í£A-Z0-9\(\)\[\]\-~ ]+?)(?=\s+\d+)'
    items = re.findall(item_pattern, text)
    
    res = []
    if biz_num_match: res.append(f"ğŸ¢ ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸: {biz_num_match.group()}")
    print(f"\n[DEBUG] ì‚¬ì—…ì: {biz_num_match.group()}") 
    if date_match: res.append(f"ğŸ“… ë‚ ì§œ: {date_match.group()}")
    if total_price_match:
        price = total_price_match.group(1).replace(" ", "").replace(",", "").strip()
        res.append(f"ğŸ’° ì´í•©ê³„: {int(price):,}ì›")
    
    if items:
        valid_items = []
        # 1. ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ëŒ€í­ ê°•í™” (OCR ì˜¤íƒ€ ëŒ€ì‘)
        stopwords = [
        # ê²°ì œ ê´€ë ¨
        'ë¬¼í’ˆê°€ì•¡', 'ê³¼ì„¸', 'ë¶€ê°€ì„¸', 'ë¶€ê°€ì„œ', 'ìƒí’ˆê°€ê²©', 'í•©ê³„', 'ê¸ˆì•¡', 'ìˆ˜ëŸ‰', 'ë‹¨ê°€',
        # ì í¬/ì£¼ì†Œ ê´€ë ¨ (ì´ë²ˆì— ì¶”ê°€!)
        'ì´ë§ˆíŠ¸', 'KMART', 'ëŒ€í•œë¯¼êµ­', 'ê³ ì–‘ì‹œ', 'ë•ì´ë™', 'ì£¼ì†Œ', 'ëŒ€í‘œì', 'ì „í™”',
        # ì•ˆë‚´ ë¬¸êµ¬ ê´€ë ¨ (ì´ë²ˆì— ì¶”ê°€!)
        'í™˜ë¶ˆ', 'í™˜ë¬¼', 'êµí™˜', 'í¸ë¦¬', 'ë“±ë¡', 'ì˜ìˆ˜ì¦', 'ë¬¸ì˜', 'ê°ì‚¬'
    ]
        
        for it in items:
            raw_name = it[1].strip()
            
            # [í•µì‹¬ ë¡œì§] ê³µë°±ì„ ì œê±°í•œ ìƒíƒœì—ì„œ ë¹„êµí•©ë‹ˆë‹¤.
            # 'í•© ê³„' -> 'í•©ê³„'ë¡œ ë³€í™˜í•´ì„œ ì²´í¬í•˜ë‹ˆê¹Œ í›¨ì”¬ ì˜ ê±¸ë ¤ìš”!
            clean_check_name = raw_name.replace(" ", "")
            
            # ë¶ˆìš©ì–´ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ íŒ¨ìŠ¤!
            if any(stop.replace(" ", "") in clean_check_name for stop in stopwords):
                continue
            
            valid_items.append(raw_name)
        
        # ì¤‘ë³µ ì œê±° (set í™œìš©)
        valid_items = list(dict.fromkeys(valid_items))

        if valid_items:
            item_str = f"ğŸ›’ í’ˆëª©: {valid_items[0]} ë“± {len(valid_items)}ê±´"
            res.append(item_str)
            print(f"[DEBUG] ìµœì¢… ì •ì œëœ í’ˆëª©ë“¤: {valid_items}")
            
    return " | ".join(res) if res else "ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨"

# ì‚¬ì§„ ì¶”ì¶œ
def extract_photo_metadata(image):
    metadata = {'width': image.width, 'height': image.height, 'camera_model': 'ì •ë³´ ì—†ìŒ', 'taken_date': 'ì •ë³´ ì—†ìŒ', 'location_address': 'ì •ë³´ ì—†ìŒ', 'lat': None, 'lng': None}
    try:
        exif_data = image._getexif()
        if exif_data:
            for tag_id, value in exif_data.items():
                tag = TAGS.get(tag_id, tag_id)
                if tag == "Model": metadata['camera_model'] = str(value).strip()
                elif tag in ["DateTime", "DateTimeOriginal"]: metadata['taken_date'] = str(value).replace(':', '-', 2)
                elif tag == "GPSInfo" and isinstance(value, dict):
                    gps_data = {GPSTAGS.get(t, t): value[t] for t in value}
                    if 'GPSLatitude' in gps_data and 'GPSLongitude' in gps_data:
                        def to_decimal(dms, ref):
                            d, m, s = [float(x) for x in dms]
                            res = d + m/60.0 + s/3600.0
                            return -res if ref in ['S', 'W'] else res
                        metadata['lat'] = to_decimal(gps_data['GPSLatitude'], gps_data['GPSLatitudeRef'])
                        metadata['lng'] = to_decimal(gps_data['GPSLongitude'], gps_data['GPSLongitudeRef'])
                        try:
                            geolocator = Nominatim(user_agent="geo_archive_v4")
                            loc = geolocator.reverse(f"{metadata['lat']}, {metadata['lng']}", language='ko')
                            if loc: metadata['location_address'] = loc.address
                        except: pass
    except: pass
    return metadata

# ---------------------------------------------------------
# pipeline: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ -> OCR ì¶”ì¶œ -> í…ìŠ¤íŠ¸ ë¶„ì„
# ---------------------------------------------------------
# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR
def get_ocr_text(img, ocr_model, is_receipt=False):
    """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì •ë°€í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # [ê¸°ë³¸ ì „ì²˜ë¦¬] ì—¬ë°± -> í™•ëŒ€ -> í‘ë°±/ì´ì§„í™”
    img_padded = cv2.copyMakeBorder(img, 40, 40, 100, 40, cv2.BORDER_CONSTANT, value=[255, 255, 255])
    h, w = img_padded.shape[:2]
    img_up = cv2.resize(img_padded, (w * 2, h * 2), interpolation=cv2.INTER_LANCZOS4)
    gray = cv2.cvtColor(img_up, cv2.COLOR_BGR2GRAY)
    processed_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

    if is_receipt:
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì•Œê³ ë¦¬ì¦˜ ì ìš© (ì˜ìˆ˜ì¦ í•œì •)
        # ì˜ë¼ì„œ ë´ì•¼ ì„¸ì„¸í•˜ê²Œ ë³´ì…ë‹ˆë‹¤. 
        # ì‹œê°„ì†Œìš”ê°€ ë” ê±¸ë¦´ ì˜ˆì •ì„ìœ¼ë¡œ ì˜ìˆ˜ì¦ í•œì •ìœ¼ë¡œ ê³„íší•¨. 
 
        ph = processed_img.shape[:2]
        win_h, overlap, texts = ph // 3, 100, []
        for i in range(3):
            start_y, end_y = max(0, i * win_h - overlap), min(ph, (i + 1) * win_h + overlap)
            res = ocr_model.ocr(processed_img[start_y:end_y, :], cls=True)
            if res and res[0]:
                for line in res[0]:
                    if line[1][0] not in texts: texts.append(line[1][0])
        return "\n".join(texts), processed_img
    else:
        # ì¼ë°˜ ëª¨ë“œ
        res = ocr_model.ocr(processed_img, cls=True)
        text = "\n".join([l[1][0] for l in res[0]]) if res and res[0] else ""
        return text, processed_img

# ë©”ì¸ í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜ 
def process_document(uploaded_file, models):
    (dit_p, dit_m, ocr, obj_p, obj_m, sum_t, sum_m, emb_m) = models
    raw_img = Image.open(io.BytesIO(uploaded_file.read()))
    orig_img = raw_img.convert("RGB")
    
    # 1. ë¶„ë¥˜
    inputs = dit_p(images=orig_img, return_tensors="pt")
    label = dit_m.config.id2label[dit_m(**inputs).logits.argmax(-1).item()].lower()
    is_receipt = any(x in label for x in ['receipt', 'invoice'])

    # 2. OCR (ì „ë‹´ í•¨ìˆ˜ í˜¸ì¶œ)
    img_cv = cv2.cvtColor(np.array(orig_img), cv2.COLOR_RGB2BGR)
    full_text, processed_img = get_ocr_text(img_cv, ocr, is_receipt)

    # 3. ë¬¸ì„œ vs ì‚¬ì§„ íŒë³„ ë° í›„ì† ì²˜ë¦¬
    is_doc = is_receipt or any(x in label for x in ['form', 'letter']) or len(full_text) > 40
    
    if is_doc:
        doc_type, structured_data = "Document", {}
        receipt_summary = extract_receipt_info(full_text)
        
        if is_receipt and receipt_summary:
            final_summary = f"ğŸ§¾ [ì˜ìˆ˜ì¦] {receipt_summary}"
        else:
            try:
                s_in = sum_t([full_text[:500]], max_length=128, return_tensors="pt", truncation=True)
                s_ids = sum_m.generate(s_in["input_ids"], num_beams=4, max_length=128)
                final_summary = sum_t.decode(s_ids[0], skip_special_tokens=True).strip()
            except: final_summary = f"{full_text[:30]}..."
        
        final_keywords = ", ".join(list(dict.fromkeys([t.form for t in kiwi.tokenize(full_text) if t.tag in ['NNG', 'NNP']]))[:10])
    else:
        doc_type = "Photo"
        processed_img = np.array(orig_img) # ì‚¬ì§„ì€ ì›ë³¸ ë°˜í™˜
        meta = extract_photo_metadata(raw_img)
        # ê°ì²´ íƒì§€ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ...
        final_summary = f"ğŸ“¸ [{meta['taken_date']}] ì´¬ì˜ ì‚¬ì§„" # ì˜ˆì‹œ ìš”ì•½
        final_keywords = "ì‚¬ì§„, ê°ì²´" # ì˜ˆì‹œ í‚¤ì›Œë“œ
        structured_data = {'exif': meta}

    embedding = emb_m.encode(full_text + " " + final_keywords).tolist()
    return (doc_type, full_text, final_summary, final_keywords, structured_data, uploaded_file.getvalue(), embedding, processed_img)



# ---------------------------------------------------------
# UI 
# ---------------------------------------------------------
st.set_page_config(layout="wide", page_title="AI Multi-Archive")
st.title("ğŸŒŸ ë©€í‹°ëª¨ë‹¬ AI í†µí•© ì•„ì¹´ì´ë¸Œ")

models = load_all_models()
t1, t2, t3, t4 = st.tabs(["ğŸ“¤ ì—…ë¡œë“œ", "ğŸ” ê²€ìƒ‰", "ğŸ“ ì•„ì¹´ì´ë¸Œ", "ğŸ“ ì§€ë„"])

with t1:
    file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=['jpg', 'png', 'jpeg'])
    if file:
        if "res" not in st.session_state or st.session_state.get("fname") != file.name:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                st.session_state.res = process_document(file, models)
                st.session_state.fname = file.name
        
        r = st.session_state.res
        col1, col2 = st.columns(2)
        col1.image(r[5], caption="ì›ë³¸")
        col2.image(r[7], caption="OCR ì „ì²˜ë¦¬ ê²°ê³¼")
        
        st.write(f"**ë¶„ë¥˜:** {r[0]} | **í‚¤ì›Œë“œ:** `{r[3]}`")
        st.info(f"**ìš”ì•½:** {r[2]}")
        
        if st.button("ğŸš€ ìµœì¢… ì €ì¥", type="primary"):
            with Session(engine) as session:
                new_doc = Document(filename=file.name, doc_type=r[0], content=r[1], 
                                   summary=r[2], keywords=r[3], 
                                   structured_data=json.dumps(r[4], ensure_ascii=False),
                                   image_data=r[5], embedding=json.dumps(r[6]))
                session.add(new_doc); session.commit()
            st.success("ì €ì¥ ì™„ë£Œ!")

with t2:
    q = st.text_input("ê²€ìƒ‰ì–´ (ê°ì²´, ì¥ì†Œ, ë‚´ìš© ë“±)")
    if q:
        with Session(engine) as session:
            results = session.exec(select(Document).where((Document.content.contains(q)) | (Document.keywords.contains(q)))).all()
            for d in results:
                with st.expander(f"ğŸ“„ {d.filename} ({d.doc_type})"):
                    sc1, sc2 = st.columns([1, 3])
                    sc1.image(d.image_data)
                    sc2.write(f"**ìš”ì•½:** {d.summary}")
                    sc2.write(f"**í‚¤ì›Œë“œ:** `{d.keywords}`")

with t3:
    with Session(engine) as session:
        items = session.exec(select(Document).order_by(Document.upload_date.desc())).all()
        for item in items:
            with st.container(border=True):
                c1, c2 = st.columns([1, 4])
                c1.image(item.image_data)
                c2.write(f"**{item.filename}** ({item.doc_type})")
                c2.caption(f"ìš”ì•½: {item.summary} | í‚¤ì›Œë“œ: {item.keywords}")
                if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{item.id}"):
                    session.delete(item); session.commit(); st.rerun()

with t4:
    st.header("ğŸ“ ì‚¬ì§„ ì´¬ì˜ ìœ„ì¹˜")
    with Session(engine) as session:
        # ì˜¤ë¥˜ í•´ê²°: st.all_docsê°€ ì•„ë‹ˆë¼ ë³€ìˆ˜ì— ë°ì´í„°ë¥¼ ë‹´ì•„ í•¨ìˆ˜ì— ì „ë‹¬í•´ì•¼ í•¨
        all_docs = session.exec(select(Document)).all()
        if all_docs:
            # display_photo_locations í•¨ìˆ˜ë¥¼ í˜¸ì¶œ (all_docs ì¸ì ì „ë‹¬)
            # (í•´ë‹¹ í•¨ìˆ˜ ë‚´ì—ì„œ lat/lng ì¶”ì¶œ ë¡œì§ì´ d.structured_dataë¥¼ íŒŒì‹±í•˜ë„ë¡ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ í•„ìš”)
            st.info(f"í˜„ì¬ {len(all_docs)}ê°œì˜ ë°ì´í„°ê°€ ì•„ì¹´ì´ë¸Œì— ìˆìŠµë‹ˆë‹¤.")